{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bab_VzWxbkkh"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "import requests\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "taXrfyuVNVE5"
      },
      "outputs": [],
      "source": [
        "ua = UserAgent()\n",
        "chrome = {'ua.agent':ua.chrome}\n",
        "url = \"https://www.1mg.com/categories/medical-devices/health-monitors/thermometers-167\"\n",
        "response = requests.get(url,headers=chrome)\n",
        "soup = BeautifulSoup(response.text, 'lxml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NVXpy8SaNVCn"
      },
      "outputs": [],
      "source": [
        "n = soup.find_all('div', {'class':'style__product-description___2XaG0'})\n",
        "names = []\n",
        "for i in n:\n",
        "  names.append(i.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uxkL1wQCNVAo"
      },
      "outputs": [],
      "source": [
        "p = soup.find_all('div',{'class':'style__price-tag___cOxYc'})\n",
        "prices = []\n",
        "for i in p:\n",
        "  prices.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wstKJF_UNU-x"
      },
      "outputs": [],
      "source": [
        "r = soup.find_all('div',{'class':'CardRatingDetail__ratings-container___2ZTSK'})\n",
        "ratings = []\n",
        "for i in r:\n",
        "  ratings.append(i.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxQP7Ng2Yx_8",
        "outputId": "29e4f2b4-c971-4779-f359-d6bf79a6b651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(len(names))\n",
        "print(len(prices))\n",
        "print(len(ratings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrytBOHYe1dt"
      },
      "source": [
        "doing everything at one shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GLf9lZ-mY9Tl"
      },
      "outputs": [],
      "source": [
        "products = soup.find_all('div',{'class':'style__product-box___liepi'})\n",
        "names = []\n",
        "prices = []\n",
        "ratings = []\n",
        "try:\n",
        "  for product in products:\n",
        "    names.append(product.find_all('div', {'class':'style__product-description___2XaG0'})[0].text)\n",
        "    prices.append(product.find_all('div',{'class':'style__price-tag___cOxYc'})[0].text)\n",
        "    ratings.append(soup.find_all('div',{'class':'CardRatingDetail__ratings-container___2ZTSK'})[0].text)\n",
        "except:\n",
        "  ratings.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC4y-THtanb-",
        "outputId": "abf8ab7a-927c-434f-9788-b1692deb1980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(len(names))\n",
        "print(len(prices))\n",
        "print(len(ratings))\n",
        "dic = {'Product Names':names, 'Prices':prices,'Ratings':ratings}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xXrvWRlfbfZr"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(dic)\n",
        "df.to_csv('output.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib20yACQezVN"
      },
      "source": [
        "multipage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_LScOm2thVM2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data scraped from 5 pages and stored in amazon_novels.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to scrape and parse data from a single page\n",
        "def scrape_amazon_page(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Your User Agent Here',\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        book_info_containers = soup.find_all('div', class_='s-result-item')\n",
        "\n",
        "        data = []\n",
        "\n",
        "        for container in book_info_containers:\n",
        "            book_name = container.find('span', class_='a-size-medium')\n",
        "            author_name = container.find('a', class_='a-size-base')\n",
        "            price = container.find('span', class_='a-price-whole')\n",
        "            review_count = container.find('span', class_='a-size-base')\n",
        "\n",
        "            # Check if the elements exist before extracting data\n",
        "            if book_name and author_name and price and review_count:\n",
        "                book_name = book_name.get_text()\n",
        "                author_name = author_name.get_text()\n",
        "                price = price.get_text()\n",
        "                review_count = review_count.get_text()\n",
        "\n",
        "                data.append([book_name, author_name, price, review_count])\n",
        "\n",
        "        return data\n",
        "    else:\n",
        "        print(\"Failed to retrieve the page.\")\n",
        "        return None\n",
        "\n",
        "# Main function to scrape data from multiple pages and store in a CSV file\n",
        "def scrape_and_store_amazon_data(url, num_pages, output_file):\n",
        "    all_data = []\n",
        "\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        page_url = f\"{url}&page={page_num}\"\n",
        "        page_data = scrape_amazon_page(page_url)\n",
        "\n",
        "        if page_data:\n",
        "            all_data.extend(page_data)\n",
        "\n",
        "    if all_data:\n",
        "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            csv_writer = csv.writer(csvfile)\n",
        "            csv_writer.writerow(['Book Name', 'Author Name', 'Price', 'Review Count'])\n",
        "            csv_writer.writerows(all_data)\n",
        "        print(f\"Data scraped from {num_pages} pages and stored in {output_file}.\")\n",
        "    else:\n",
        "        print(\"No data to store.\")\n",
        "\n",
        "# Set the URL, number of pages to scrape, and output file name\n",
        "url = 'https://www.amazon.in/novels/s?k=novels'\n",
        "num_pages = 5  # You can adjust this to scrape more or fewer pages\n",
        "output_file = 'amazon_novels.csv'\n",
        "\n",
        "# Call the main scraping function\n",
        "scrape_and_store_amazon_data(url, num_pages, output_file)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
