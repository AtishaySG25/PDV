{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fake-useragent in c:\\users\\atishay sg\\anaconda3\\envs\\aimlsem1\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: importlib-resources>=5.0 in c:\\users\\atishay sg\\anaconda3\\envs\\aimlsem1\\lib\\site-packages (from fake-useragent) (6.0.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\atishay sg\\anaconda3\\envs\\aimlsem1\\lib\\site-packages (from importlib-resources>=5.0->fake-useragent) (3.16.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake user agent\n",
    "user_agent = UserAgent()\n",
    "\n",
    "# Create a fake user agent\n",
    "main_url = 'https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/'\n",
    "# Make a GET request to the main URL\n",
    "page = requests.get(main_url, headers={'user-agent':user_agent.chrome})\n",
    "# Parse the HTML content of the response\n",
    "soup=BeautifulSoup(page.content,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing all relative links\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n",
      "#\n",
      "https://penguin.co.in/books/fiction/fantacy-horror-science-fiction/#\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://penguin.co.in/'\n",
    "\n",
    "# Create an empty list to store the extracted links\n",
    "link_list = []\n",
    "\n",
    "# Find all 'div' elements with class 'book-card'\n",
    "all_divs = soup.find_all('div', class_ = 'book-card')\n",
    "\n",
    "# Extract and append links to link_list\n",
    "print(\"Printing all relative links\")  \n",
    "for div in all_divs:\n",
    "    a = div.a['href']\n",
    "    link_list.append(a)\n",
    "\n",
    "for a in link_list:\n",
    "    print(a)\n",
    "    print(main_url + a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing all absolute links\n",
      "\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n",
      "https://penguin.co.in/#\n"
     ]
    }
   ],
   "source": [
    "# Print the absolute link by joining base_url with the relative link\n",
    "print(\"Printing all absolute links\\n\")\n",
    "for div in all_divs:\n",
    "    print(base_url + div.a['href']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "all_links = [base_url + div.a['href'] for div in all_divs]\n",
    "\n",
    "# Iterate through the extracted links\n",
    "for link in all_links:\n",
    "    inner_page = requests.get(link, headers= {'user_agent': user_agent.chrome})\n",
    "    inner_soup = BeautifulSoup(inner_page.content, 'lxml')\n",
    "\n",
    "    div = inner_soup.find('div', class_ = 'filter-active')\n",
    "\n",
    "# Find all paragraph ('p') tags in the 'soup' object\n",
    "    p_tags = soup.find_all('p')\n",
    "    \n",
    " # Extract and print links from 'a' tags within 'p' tags, if they exist\n",
    "    info_links = [base_url + p.a['href'] for p in p_tags if p.a]\n",
    "    print(info_links)\n",
    "    break\n",
    "\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all_links = [base_url + div.a[\\'href\\'] for div in all_divs]\\n\\n# Iterate through the extracted links\\nfor link in all_links:\\n    inner_page = requests.get(link, headers= {\\'user_agent\\': user_agent.chrome})\\n    inner_soup = BeautifulSoup(inner_page.content, \\'lxml\\')\\n\\n    div = inner_soup.find(\\'div\\', class_ = \\'filter-active\\')\\n\\n    if div:\\n        para_tags = div.find_all(\\'h4\\')\\n        para_contents = [tag.get_text() for tag in para_tags]\\n        print(para_contents)\\n    else:\\n        print(\"No div element found\")\\n\\n    info_links = [base_url + p.a[\\'href\\'] for p in div.p.find_all(\\'p\\')]\\n    print(info_links)\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''all_links = [base_url + div.a['href'] for div in all_divs]\n",
    "\n",
    "# Iterate through the extracted links\n",
    "for link in all_links:\n",
    "    inner_page = requests.get(link, headers= {'user_agent': user_agent.chrome})\n",
    "    inner_soup = BeautifulSoup(inner_page.content, 'lxml')\n",
    "\n",
    "    div = inner_soup.find('div', class_ = 'filter-active')\n",
    "\n",
    "    if div:\n",
    "        para_tags = div.find_all('h4')\n",
    "        para_contents = [tag.get_text() for tag in para_tags]\n",
    "        print(para_contents)\n",
    "    else:\n",
    "        print(\"No div element found\")\n",
    "\n",
    "    info_links = [base_url + p.a['href'] for p in div.p.find_all('p')]\n",
    "    print(info_links)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIMLSem1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
